{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TESTING"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tugberk/DataspellProjects/redpill/rpenv/lib/python3.7/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import redditcleaner\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                body\n",
      "0  We've known about the specter of censorship as...\n",
      "1  Hello I just started reading about TRP theorie...\n",
      "2  **CorporateLand: Don’t Kill the Job v. Quiet Q...\n",
      "3  **Schrödinger’s Girlfriend**\\n\\n*“Girls are ne...\n",
      "4  Hola amigos!! This is my current gameplan.  Pl...\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "df =  pd.read_csv(\"data/redpill_sample_500.csv\",\n",
    "                  usecols=[\"body\"],\n",
    "                  na_filter=\"NaN\",\n",
    "                  na_values=\"NaN\").dropna()\n",
    "\n",
    "df[\"body\"].dropna(inplace=True)\n",
    "#df['post_preprocessed'] = df.apply(preprocess_text).str.lower()\n",
    "#\n",
    "#print('lemming...')\n",
    "#nltk.download('wordnet')\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#df['post_final'] = df['post_preprocessed'].apply(lambda post: lemmatize_words(post, lemmatizer))\n",
    "#\n",
    "#print('remove stopwors...')\n",
    "#\n",
    "#nltk.download('stopwords')\n",
    "#swords = set(stopwords.words('english'))\n",
    "#\n",
    "#df['post_final'] = df['post_preprocessed'].apply(lambda post: remove_stopwords(post, swords))\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "0      We've known about the specter of censorship as...\n1      Hello I just started reading about TRP theorie...\n2      **CorporateLand: Don’t Kill the Job v. Quiet Q...\n3      **Schrödinger’s Girlfriend**\\n\\n*“Girls are ne...\n4      Hola amigos!! This is my current gameplan.  Pl...\n                             ...                        \n351    Recently my mate and I started a small daily h...\n352    Listen when you’ve been on the road as long as...\n353    Day 7 of no fap, 6 months off weed, day 4 of h...\n354    This is from our elder statesman and a founder...\n355    **The Importance of Manliness.**\\n\\nIt’s impor...\nName: body, Length: 348, dtype: object"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "text_cleaned = redditcleaner.clean(df[\"body\"][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'Hello I just started reading about TRP theories. I am still at the early chapters of the Handbook and cannot quite pin where this situation stands: I met a girl at a festival, we exchanged instagrams and now I am seeing that 80% of her stories are heavy drinking and partying. I tease her about that for a while and yesterday I asked her \"I can\\'t remember, but are you studying something at the moment?\" My intention was to spin this around college parties and invite myself to one. She answered with \"No, I don\\'t study anything\", but some time has passed and I was out of the mood for texting and into the mood for going to bed, so I just decided I will answer in the morning. This morning I just left her on \"seen\" and texted nothing back. I realized that I don\\'t feel like continuing the topic and really can\\'t bother with coming up with something witty to say. I think I will just text her in a few days about some other topic. How would you classify the current situation? I think it is qualification/rapport break, but I also want to hear more experienced opinion, to check if I am understand correctly what I read in the Handbook.'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0      We've known about the specter of censorship as...\n1      Hello I just started reading about TRP theorie...\n2      **CorporateLand: Don’t Kill the Job v. Quiet Q...\n3      **Schrödinger’s Girlfriend**\\n\\n*“Girls are ne...\n4      Hola amigos!! This is my current gameplan.  Pl...\n                             ...                        \n351    Recently my mate and I started a small daily h...\n352    Listen when you’ve been on the road as long as...\n353    Day 7 of no fap, 6 months off weed, day 4 of h...\n354    This is from our elder statesman and a founder...\n355    **The Importance of Manliness.**\\n\\nIt’s impor...\nName: body, Length: 348, dtype: object"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"body\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cleaned_text = []\n",
    "for row in range(len(df[\"body\"])):\n",
    "    try:\n",
    "        cleaned_text.append(redditcleaner.clean(df['body'][row]))\n",
    "    except TypeError:\n",
    "        continue\n",
    "    except KeyError:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "340"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(cleaned_text, columns=[\"body\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:12: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:12: DeprecationWarning: invalid escape sequence \\S\n",
      "<>:16: DeprecationWarning: invalid escape sequence \\s\n",
      "/var/folders/59/t9r6dbl91y18h_j87198fnl40000gn/T/ipykernel_19649/3997932686.py:12: DeprecationWarning: invalid escape sequence \\S\n",
      "  email_pattern = re.compile('\\S*@\\S*\\s?')\n",
      "/var/folders/59/t9r6dbl91y18h_j87198fnl40000gn/T/ipykernel_19649/3997932686.py:16: DeprecationWarning: invalid escape sequence \\s\n",
      "  return re.sub('\\s+', ' ', text)\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    \" removes urls\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    \" removes html tags\"\n",
    "    html_pattern = re.compile('')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emails(text):\n",
    "    email_pattern = re.compile('\\S*@\\S*\\s?')\n",
    "    return email_pattern.sub(r'', text)\n",
    "\n",
    "def remove_new_line(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    return re.sub(\"[^A-Za-z]+\", ' ', str(text))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    #t = remove_urls(text)\n",
    "    t = remove_html(text)\n",
    "    #t = remove_emails(t)\n",
    "    t = remove_new_line(t)\n",
    "    t = remove_non_alpha(t)\n",
    "    return t\n",
    "\n",
    "def lemmatize_words(text, lemmatizer):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stopwords])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemming...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tugberk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove stopwors...\n",
      "                                                body  \\\n",
      "0  We've known about the specter of censorship as...   \n",
      "1  Hello I just started reading about TRP theorie...   \n",
      "2  CorporateLand: Don’t Kill the Job v. Quiet Qui...   \n",
      "3  Schrödinger’s Girlfriend “Girls are never sing...   \n",
      "4  Hola amigos This is my current gameplan. Pleas...   \n",
      "\n",
      "                                   post_preprocessed  \\\n",
      "0  we ve known about the specter of censorship as...   \n",
      "1  hello i just started reading about trp theorie...   \n",
      "2  corporateland don t kill the job v quiet quitt...   \n",
      "3  schr dinger s girlfriend girls are never singl...   \n",
      "4  hola amigos this is my current gameplan please...   \n",
      "\n",
      "                                          post_final  \n",
      "0  known specter censorship red pill censorship f...  \n",
      "1  started reading trp theories early chapters ha...  \n",
      "2  corporateland kill job v quiet quitting two di...  \n",
      "3  schr dinger girlfriend single relationships sa...  \n",
      "4  hola amigos current gameplan please note newbi...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tugberk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "\n",
    "clean_df['post_preprocessed'] = clean_df['body'].apply(preprocess_text).str.lower()\n",
    "\n",
    "print('lemming...')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_df['post_final'] = clean_df['post_preprocessed'].apply(lambda post: lemmatize_words(post, lemmatizer))\n",
    "\n",
    "print('remove stopwors...')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "swords = stopwords.words('english')\n",
    "swords.extend(['from', 'woman',\n",
    "               'women','men',\n",
    "               'man','like',\n",
    "               'even','go','one',\n",
    "               'topic','morning','something',\n",
    "               'girl','girls', 'guy', 'give',\n",
    "               'would','things',\n",
    "               'know','really',\n",
    "               'never','hello',\n",
    "               'first','every','lot','much','make',\n",
    "               'want','life','good','x',\n",
    "               'people','going','getting','saying',\n",
    "               'day', 'get',\n",
    "               'say','way',\n",
    "               'guys', 'think',\n",
    "               'feel','also',\n",
    "               'see', 'need', 'thing',\n",
    "               'b', 'many',\n",
    "               'take','let',\n",
    "               'still', 'fuck',\n",
    "               'could','may',\n",
    "               'said', 'back', 'better', 'always', 'right',\n",
    "               'time', 'shit','around', 'fucking', 'work',\n",
    "               'sexual', 'got', 'years', 'post', 'point',\n",
    "               'self', 'world',\n",
    "               'ever','trying', 'talking',\n",
    "               'long', 'year', 'tell',\n",
    "               'another', 'someone', 'look',\n",
    "               'everything', 'anything', 'nothing',\n",
    "               'find', 'enough', 'actually',\n",
    "               'keep', 'well', 'best', 'number',\n",
    "               'sex', 'game', 'mind',\n",
    "               'talk', 'new', 'next', 'love'])\n",
    "swords =set(swords)\n",
    "clean_df['post_final'] = clean_df['post_preprocessed'].apply(lambda post: remove_stopwords(post, swords))\n",
    "print(clean_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  body  \\\n280  It's been a long time bros, and I doubt anyone...   \n121  I’ll try to summarize it in a way that doesn't...   \n175  relevant to the men who insist on getting marr...   \n234  Introduction: What is the Whorely Yuga? The Wh...   \n\n                                     post_preprocessed  \\\n280  it s been a long time bros and i doubt anyone ...   \n121  i ll try to summarize it in a way that doesn t...   \n175  relevant to the men who insist on getting marr...   \n234  introduction what is the whorely yuga the whor...   \n\n                                            post_final  \n280  bros doubt anyone remembers posted times bothe...  \n121  try summarize support condemn either since sor...  \n175  relevant insist married interested divorced ma...  \n234  introduction whorely yuga whorely yuga last cy...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>post_preprocessed</th>\n      <th>post_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>280</th>\n      <td>It's been a long time bros, and I doubt anyone...</td>\n      <td>it s been a long time bros and i doubt anyone ...</td>\n      <td>bros doubt anyone remembers posted times bothe...</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>I’ll try to summarize it in a way that doesn't...</td>\n      <td>i ll try to summarize it in a way that doesn t...</td>\n      <td>try summarize support condemn either since sor...</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>relevant to the men who insist on getting marr...</td>\n      <td>relevant to the men who insist on getting marr...</td>\n      <td>relevant insist married interested divorced ma...</td>\n    </tr>\n    <tr>\n      <th>234</th>\n      <td>Introduction: What is the Whorely Yuga? The Wh...</td>\n      <td>introduction what is the whorely yuga the whor...</td>\n      <td>introduction whorely yuga whorely yuga last cy...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.sample(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "posts = [x.split(' ') for x in clean_df['post_final']]\n",
    "#p2 = []\n",
    "#for i in range(len(posts)):\n",
    " #   p2.append(posts[posts[i] not in ['women','men','man','like','even','go','one', 'topic','morning','something']])\n",
    "id2word = corpora.Dictionary(posts)\n",
    "corpus_tf = [id2word.doc2bow(text) for text in posts]\n",
    "#print(corpus_tf[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus_tf)\n",
    "corpus_tfidf = tfidf[corpus_tf]\n",
    "#print(corpus_tfidf[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1136559547404297\n"
     ]
    }
   ],
   "source": [
    "model = LdaMulticore(corpus=corpus_tf,id2word = id2word, num_topics = 5,\n",
    "                    chunksize=50,\n",
    "                     random_state = 0)\n",
    "\n",
    "coherence = CoherenceModel(model = model, texts = posts, dictionary = id2word, coherence = 'u_mass')\n",
    "\n",
    "print(coherence.get_coherence())\n",
    "#print(model.show_topics())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 \n",
      " [('approach', 0.00256927), ('high', 0.0019090173), ('night', 0.0018843431), ('frame', 0.0018474185), ('social', 0.0018260335), ('different', 0.0018228295), ('wants', 0.0017581327), ('alpha', 0.0017472613), ('might', 0.0017390576), ('little', 0.0017377133)]\n",
      "topic 1 \n",
      " [('try', 0.002071226), ('without', 0.0019180438), ('social', 0.0018719806), ('us', 0.0018109385), ('relationship', 0.0017857851), ('partners', 0.001703858), ('else', 0.0016711933), ('alpha', 0.001654076), ('red', 0.0016463625), ('since', 0.0016174719)]\n",
      "topic 2 \n",
      " [('us', 0.0024020174), ('try', 0.0023717063), ('red', 0.002290104), ('approach', 0.002251057), ('start', 0.0021032104), ('pill', 0.0018654057), ('use', 0.0018206816), ('come', 0.0018175311), ('relationship', 0.0018103085), ('friends', 0.001782004)]\n",
      "topic 3 \n",
      " [('made', 0.00214781), ('attractive', 0.0021055061), ('red', 0.0019165752), ('money', 0.0019107823), ('pill', 0.0019084745), ('wanted', 0.0018895183), ('approach', 0.0018783739), ('making', 0.0018472612), ('must', 0.0018304787), ('friends', 0.0017959399)]\n",
      "topic 4 \n",
      " [('alpha', 0.0022357882), ('relationship', 0.0020831823), ('end', 0.0020738197), ('pill', 0.0020317226), ('friend', 0.0018706205), ('makes', 0.0018437821), ('money', 0.001837819), ('hard', 0.0018240939), ('making', 0.0017719996), ('start', 0.0017515356)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"topic {} \\n\".format(i),model.show_topic(topicid=i,topn=10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "     dominant_topic  perc_contribution  \\\n147               2              0.924   \n262               3              0.723   \n71                2              0.991   \n233               0              0.849   \n238               3              0.877   \n\n                                        topic_keywords  \\\n147  us, try, red, approach, start, pill, use, come...   \n262  made, attractive, red, money, pill, wanted, ap...   \n71   us, try, red, approach, start, pill, use, come...   \n233  approach, high, night, frame, social, differen...   \n238  made, attractive, red, money, pill, wanted, ap...   \n\n                                                  post  \n147  “I AM ABOVE HER” FRAME AT ALL TIMES - treat he...  \n262  A few nights ago, I recorded a podcast with a ...  \n71   A lot of strippers and escorts have boyfriends...  \n233  Humans evolved to feel like the most important...  \n238  What is a full body erection? A full body erec...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dominant_topic</th>\n      <th>perc_contribution</th>\n      <th>topic_keywords</th>\n      <th>post</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>147</th>\n      <td>2</td>\n      <td>0.924</td>\n      <td>us, try, red, approach, start, pill, use, come...</td>\n      <td>“I AM ABOVE HER” FRAME AT ALL TIMES - treat he...</td>\n    </tr>\n    <tr>\n      <th>262</th>\n      <td>3</td>\n      <td>0.723</td>\n      <td>made, attractive, red, money, pill, wanted, ap...</td>\n      <td>A few nights ago, I recorded a podcast with a ...</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>2</td>\n      <td>0.991</td>\n      <td>us, try, red, approach, start, pill, use, come...</td>\n      <td>A lot of strippers and escorts have boyfriends...</td>\n    </tr>\n    <tr>\n      <th>233</th>\n      <td>0</td>\n      <td>0.849</td>\n      <td>approach, high, night, frame, social, differen...</td>\n      <td>Humans evolved to feel like the most important...</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>3</td>\n      <td>0.877</td>\n      <td>made, attractive, red, money, pill, wanted, ap...</td>\n      <td>What is a full body erection? A full body erec...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {'dominant_topic':[], 'perc_contribution':[], 'topic_keywords':[]}\n",
    "\n",
    "for i, row in enumerate(model[corpus_tf]):\n",
    "    #print(i)\n",
    "    row = sorted(row, key=lambda x: x[1], reverse=True)\n",
    "    #print(row)\n",
    "    for j, (topic_num, prop_topic) in enumerate(row):\n",
    "        wp = model.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        data_dict['dominant_topic'].append(int(topic_num))\n",
    "        data_dict['perc_contribution'].append(round(prop_topic, 3))\n",
    "        data_dict['topic_keywords'].append(topic_keywords)\n",
    "        #print(topic_keywords)\n",
    "        break\n",
    "\n",
    "df_topics = pd.DataFrame(data_dict)\n",
    "\n",
    "contents = pd.Series(posts)\n",
    "\n",
    "df_topics['post'] = clean_df['body']\n",
    "df_topics.sample(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 \n",
      " 250    The most common and at the same time simplest ...\n",
      "254    I sit at my desktop computer having NOT been t...\n",
      "200    After talking to most guys, you'll notice a th...\n",
      "214    Post Quote: \"What-eva, I do what I want\" - Eri...\n",
      "43     Because I think FRs should be instructive, the...\n",
      "Name: post, dtype: object \n",
      "\n",
      "topic 1 \n",
      " 277    Diwali (the festival of lights, if you're inte...\n",
      "229    It is the New Year’s Eve, or for others it is ...\n",
      "12     M35, Married 1 year. My wife, who is generally...\n",
      "164    TLDR: Is it wise to play the 36 questions game...\n",
      "33     https://youtu.be/DGbf-aK3YxU Something that ca...\n",
      "Name: post, dtype: object \n",
      "\n",
      "topic 2 \n",
      " 328    Introduction: Strenh is everything. If you’re ...\n",
      "50     I assert that the greatest irony of all is the...\n",
      "285    Your Dick Is A Gift To Women, Stop Devaluing Y...\n",
      "219    &x200B; Attraction. This single concept gets m...\n",
      "14     You want to meet new women but you have no ide...\n",
      "Name: post, dtype: object \n",
      "\n",
      "topic 3 \n",
      " 265    Today apparently is one of those days I woke u...\n",
      "77     Many cultures have a myth where an uncivilized...\n",
      "156    TD; DR - Some higher-up people instinctively k...\n",
      "239    OK, I should clarify the title. Understanding ...\n",
      "181    I've learned that the best openers are the mos...\n",
      "Name: post, dtype: object \n",
      "\n",
      "topic 4 \n",
      " 80     I needed a place to write all this down so I’m...\n",
      "114    We all know a big part of TRP philosophy is be...\n",
      "151    Hello everyone, today I'd like to the discuss ...\n",
      "337    I've always felt that my life had a path. I kn...\n",
      "19     Recently I read the book  () It was pretty dec...\n",
      "Name: post, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"topic {} \\n\".format(i),df_topics[df_topics[\"dominant_topic\"] == i][\"post\"].sample(5), \"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}